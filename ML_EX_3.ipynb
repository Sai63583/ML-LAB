{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoZiCnk+oW/z+kpHT7cczJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sai63583/ML-LAB/blob/main/ML_EX_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, attribute=None, label=None):\n",
        "        self.attribute = attribute  # Attribute split at this node\n",
        "        self.children = {}          # Dictionary to store children nodes\n",
        "        self.label = label          # Class label if leaf node\n",
        "\n",
        "def entropy(data):\n",
        "    \"\"\"\n",
        "    Calculate the entropy of a dataset.\n",
        "    \"\"\"\n",
        "    n = len(data)\n",
        "    if n == 0:\n",
        "        return 0\n",
        "\n",
        "    counts = Counter(data)\n",
        "    probs = [count / n for count in counts.values()]\n",
        "    entropy_val = -sum(p * math.log2(p) for p in probs if p != 0)\n",
        "    return entropy_val\n",
        "\n",
        "def information_gain(data, attribute_values, attribute_index):\n",
        "    \"\"\"\n",
        "    Calculate the information gain of an attribute.\n",
        "    \"\"\"\n",
        "    n = len(data)\n",
        "    attribute_entropy = 0\n",
        "\n",
        "    for value in attribute_values:\n",
        "        subset = [example for example in data if example[attribute_index] == value]\n",
        "        subset_entropy = entropy([example[-1] for example in subset])\n",
        "        attribute_entropy += len(subset) / n * subset_entropy\n",
        "\n",
        "    return entropy([example[-1] for example in data]) - attribute_entropy\n",
        "\n",
        "def choose_best_attribute(data, attributes):\n",
        "    \"\"\"\n",
        "    Choose the best attribute to split on based on information gain.\n",
        "    \"\"\"\n",
        "    attribute_values = set(data[:, 0])\n",
        "    best_gain = -1\n",
        "    best_attribute = None\n",
        "\n",
        "    for index, attribute in enumerate(attributes):\n",
        "        gain = information_gain(data, attribute_values, index)\n",
        "        if gain > best_gain:\n",
        "            best_gain = gain\n",
        "            best_attribute = attribute\n",
        "\n",
        "    return best_attribute\n",
        "\n",
        "def split_data(data, attribute_index, value):\n",
        "    \"\"\"\n",
        "    Split the data based on a specific attribute value.\n",
        "    \"\"\"\n",
        "    return data[data[:, attribute_index] == value]\n",
        "\n",
        "def majority_vote(data):\n",
        "    \"\"\"\n",
        "    Return the most common class label in the data.\n",
        "    \"\"\"\n",
        "    counts = Counter(data[:, -1])\n",
        "    return counts.most_common(1)[0][0]\n",
        "\n",
        "def ID3(data, attributes):\n",
        "    \"\"\"\n",
        "    Build a decision tree using the ID3 algorithm.\n",
        "    \"\"\"\n",
        "    # If all examples have the same label, return a leaf node\n",
        "    if len(set(data[:, -1])) == 1:\n",
        "        return Node(label=data[0][-1])\n",
        "\n",
        "    # If there are no attributes left, return a leaf node with the majority vote\n",
        "    if len(attributes) == 0:\n",
        "        return Node(label=majority_vote(data))\n",
        "\n",
        "    # Choose the best attribute to split on\n",
        "    best_attribute = choose_best_attribute(data, attributes)\n",
        "\n",
        "    # Create a new node with the chosen attribute\n",
        "    node = Node(attribute=best_attribute)\n",
        "\n",
        "    # Recursively build the tree for each possible value of the chosen attribute\n",
        "    attribute_index = attributes.index(best_attribute)\n",
        "    attribute_values = set(data[:, attribute_index])\n",
        "    for value in attribute_values:\n",
        "        subset = split_data(data, attribute_index, value)\n",
        "        if len(subset) == 0:\n",
        "            node.children[value] = Node(label=majority_vote(data))\n",
        "        else:\n",
        "            node.children[value] = ID3(subset, [attr for attr in attributes if attr != best_attribute])\n",
        "\n",
        "    return node\n",
        "\n",
        "def classify(tree, instance):\n",
        "    \"\"\"\n",
        "    Classify a new instance using the decision tree.\n",
        "    \"\"\"\n",
        "    if tree.label is not None:\n",
        "        return tree.label\n",
        "\n",
        "    attribute_value = instance[tree.attribute]\n",
        "    if attribute_value not in tree.children:\n",
        "        return 'unknown'\n",
        "\n",
        "    return classify(tree.children[attribute_value], instance)\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# Toy dataset: weather data\n",
        "data = np.array([\n",
        "    ['Sunny', 'Hot', 'High', 'Weak', 'No'],\n",
        "    ['Sunny', 'Hot', 'High', 'Strong', 'No'],\n",
        "    ['Overcast', 'Hot', 'High', 'Weak', 'Yes'],\n",
        "    ['Rain', 'Mild', 'High', 'Weak', 'Yes'],\n",
        "    ['Rain', 'Cool', 'Normal', 'Weak', 'Yes'],\n",
        "    ['Rain', 'Cool', 'Normal', 'Strong', 'No'],\n",
        "    ['Overcast', 'Cool', 'Normal', 'Strong', 'Yes'],\n",
        "    ['Sunny', 'Mild', 'High', 'Weak', 'No'],\n",
        "    ['Sunny', 'Cool', 'Normal', 'Weak', 'Yes'],\n",
        "    ['Rain', 'Mild', 'Normal', 'Weak', 'Yes'],\n",
        "    ['Sunny', 'Mild', 'Normal', 'Strong', 'Yes'],\n",
        "    ['Overcast', 'Mild', 'High', 'Strong', 'Yes'],\n",
        "    ['Overcast', 'Hot', 'Normal', 'Weak', 'Yes'],\n",
        "    ['Rain', 'Mild', 'High', 'Strong', 'No']\n",
        "])\n",
        "\n",
        "# Attribute names\n",
        "attributes = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n",
        "\n",
        "# Build the decision tree\n",
        "tree = ID3(data, attributes)\n",
        "\n",
        "# Test the decision tree\n",
        "test_instance = {'Outlook': 'Sunny', 'Temperature': 'Cool', 'Humidity': 'High', 'Wind': 'Strong'}\n",
        "prediction = classify(tree, test_instance)\n",
        "print(f\"The prediction for instance {test_instance} is: {prediction}\")\n",
        "\n",
        "output:\n",
        "The prediction for instance {'Outlook': 'Sunny', 'Temperature': 'Cool', 'Humidity': 'High', 'Wind': 'Strong'} is: unknown\n"
      ],
      "metadata": {
        "id": "dNpgZ0g_bN7Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}